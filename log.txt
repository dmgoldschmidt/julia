Fri 19 Feb 2021 04:22:24 PM PST
I've been learning julia for the past two or three weeks.  So far, I have a working sort.jl, CommandLine.jl,
VarArray.jl, and fts_flowsets.jl:

sort.jl: similar to Heap.h but it implements a TableComp functor which does lexicographic ordering in tables( which
are of type Array[][].

CommandLine.jl is similar to the C++ version, but its convenience function getvals inputs a julia Dict defaults,
then parses an input CommandLine (which is an Array::strings) and uses it to modify the defaults.

VarArray.jl automatically extends the length of a VarArray up to a specified maximum.  Right now, the maximum
is 10 times the starting length, but this could be changed to a user-specified value.

fts_flowsets.jl reads a raw wsa file in either text or .gz format and outputs a table of feature vectors, consisting
of four pseudo-median values for IAT, DUR, BYTES_IN and BYTES_OUT, as well as the connection ident, the threat level,
and the no. of records found for the given connection.  This code ran successfully on a full day (07-01) of wsa data,
producing over 2 million fts feature vectors, with 249,298 consisting of at least 10 records.

Sat 20 Feb 2021 11:48:10 AM PST
It took me two hours this morning to figure out how to convert the Array that is returned from "split" to a Tuple,
which will supposedly improve performance.  Obvious things, like " row = tuple(split(line,"|")) " don't work. It
turns out that you need  " row = tuple(split(line,"|")...) " which seems ridiculous.  I also tried logical things
like "row = tuple(split(line,"|")[1:end]) " and probably twenty others.  Coping with the sometimes bizarre syntax
together with the shitty error messages is quite a challenge.


Sat 20 Feb 2021 05:15:26 PM PST
Working now on features2cdf.jl.  The plan:
1. read the feature file into a big table
2. create a col = IndexPair[]
3. for each cdf-able column in the table:
   a. push an IndexPair for each entry into col (the first time -- after that, the space in col already exists)
   b. sort col
   c. for i in 1:n = length(col); replace table[j][col[i].index] with i/n
4. write the table back out
   
Sun 21 Feb 2021 08:27:25 AM PST
Notes for sort:
1. Change TableComp to LexOrder (more descriptive)
2. Change all comparison functors to return -1,0,or 1 depending on >,=,<

Sun 21 Feb 2021 05:10:39 PM PST
So if A = Matrix(m,n) then A[:,j] is a column vector consisting of the jth column of A.  BUT
if we change A[:,j] by, say, sorting it, this doesn't necessarily change A.  so e.g. the code

A = Matrix(m,n)
heapsort(A[:,1])

doesn't change A.  Instead, we need
A = Matrix(m,n)
v = A[:,1]
heapsort(v)
A[:1] = v

which is of course ridiculous, but there you are.

Tue 23 Feb 2021 03:29:43 PM PST
Here are my comments for the latest commit:
util.jl:
initial commit.  Moved VarArray here and deleted VarArray.jl.  Moved is_dotted_quad and tryopen as well

fts_flowsets.jl:
implemented @isdefined for sort.jl, util.jl, CommandLine.jl,
added fqdn to struct Flowset
added sldn or "(missing)" to output
now using tryopen for opening files

sort.jl:
changed "TableComp" to "LexComp"
implemented @isdefined

features2cdf:
implemented @isdefined, and moved is_dotted_quad to util.jl

clusters.jl:
initial commit.

I ran PEcluster with four parameters from the wsa data on the output of fts_features.jl -> features2cdf.jl on the aws.
Got 64 clusters on about 250K data.  Each fts_flowset now contains:  ident,(4 features), fqdn, threat, descr.
clusters.jl processes the cluster file (nominally clusters.out) from PEcluster by sorting on (cluster_no, prob)
and then printing one line for each cluster: cluster_no, #connections, average prob.
(just added SLDN to the list, but I'm going to change it to FQDN).

Thu 04 Mar 2021 09:15:30 AM PST
Wrote and debugged (I hope!) predict.jl, which inputs 5-dimensional clustering on the wsa cdf data.  The fifth
variable is the threat value, and the idea is to compute the conditional pdf on the threat using the other four
variables and then score the threat.  So far, the results have been fairly disappointing.  There's a counter-intuitive
result that I get better answers with fewer states (clusters).  This suggests that there's a bug somewhere.  Right
now, I'm using the conditional distribution from the full n-state model, but there's nothing to prevent just
choosing the best state and getting prediction from that state only.  I'll try that next.  I also want to
experiment with a few extra derived variables, like inbytes/(inbytes+outbytes) and a language model score on
the fqdn.

Fri 05 Mar 2021 04:50:24 PM PST
Making some mods to predict, but so far I have no joy.  The scoring rate is very bad for all models so far.  I'm
about to compute simple rms error.  I also discovered that Vector{Float64}(undef, dim) is NOT a good way to initialize
a vector.  Turns out you should use fill(0.0,dim) or fill(0.0, (nrows,ncols)).  This is more bizarro julia usage.

Sat 06 Mar 2021 04:50:08 PM PST
OK, I'm getting basically garbage results back on threat prediction.  There's two possibilities:  1) a bug,
2) there's no soup in the four predictors towards the threat number.  To disambiguate these, my next move
will be to generate fake test data with four predictors consisting of random no.s in [0,1], and a fifth
goal which is the sum of all four plus a small amount of noise.  Then I'll run PE cluster on the fake data,
and then predict.jl.  I *must* see a strong ability to predict the sum from the other four.  Before I start,
I want to review the connection between the conditional distribution and linear regression.

Sat 13 Mar 2021 09:42:00 PM PST
I essentially rewrote predict so that I can test some of the code by building a 1-state model directly from
the simulated (or real, I guess) data and then predicting with it.  Once I get that working, I can go back to
the multi-state model on the simulated data and finally on the real data.  Next step:  debugging the new code.
Along the way, I implemented the matrix Welford algorithm, and my own version of tryparse, called myparse, which
instead of returning Nothing on an error, prints an error message and exits.

Mon 15 Mar 2021 10:16:34 PM PDT
It looks like predict.jl is running OK, both in test mode (1 state from the sim data) and from a model file built
from the sim data.  I fixed quite a few minor glitches.  One more test, using 2 states from the model file, which
should give substantially the same results.  Then I'll re-build the real model from the wsa data and rerun predict.
We'll see if there's any soup.

Wed 17 Mar 2021 05:01:20 PM PDT
I think I wasn't really doing prediction right, so I've changed predict.jl to compute a per data point per state
weight based on the pdf of the four-long predictor.  I find the "best state at time t" using this, and compute
a prediction using that state.  I also make a "soft choice" by doing a weighted sum across all the states
on the conditional mean and variance.  So far, the results are terrible.

Thu 18 Mar 2021 05:43:12 PM PDT I built an 8-state test model (4 predictors +
threat) using simulated data, which involved an 8x4 random matrix M.  At time
t, I print M[t%8+1,:] followed by the sum of the four random no.s, but I apply
a small amount of white noise to all five values before printing.  If I run
PEcluster on this data, I get an 8-state model which nails the 8 random
states.  I then use the model+data together with predict.jl and as expected, I
get very strong prediction scores.  So I don't think either program has bugs,
it's just that there is no discernable linear-ish relationship between the
actual 4 predictors and the threat.  I also got the julia notebook running
again.  For some reason, I couldn't reproduce my earlier success using Ijulia.
But I somehow blundered into successfully importing Conda, figuring out where
the notebook execution point was, and installing an alias "notebook" in
.bashrc.  So I can get a julia notebook going by just typing "notebook" at the
emacs shell prompt.  Unfortunately, there's something hokey with
markdown/latex -- I tried to install equation numbering, but it didn't work,
and not only that, my version doesn't recognize \begin{equation}, although
\begin{align} does work.  Go figure.  See ~/temp for some code that I can
supposedly execute in a notebook cell, but it doesn't seem to work (with
either \begin{equation} or \begin{align}.  So at this point, serious use of
latex in the notebook seems to be a non-starter.

Sat 01 May 2021 03:32:14 PM PDT
I've been making log entries in the Bridgery log for a while, but I thought
I'd come back here to record stats.jl, which is a direct recoding of the NR
routines needed for KScomp and Chisq.  Everything checks in a direct test
*except* that gammq(a,x) for x >= 1+a is off after 3 or 4 decimal places. So
the discrepancy is with the gcf code, which is a continued fraction calculation.
I'm not sure why it's off, but for now I'm not worrying about it.  I did check
gammq(16,28) with an online calculator, and the NR result checks very closely.

Sat 08 May 2021 09:49:48 AM PDT
Found a nasty little bug in read_model.  The inverse cholesky was not
being parsed correctly.  Looks OK now.

Wed May 19 10:40:37 2021
I experimented with applying the log function to the tail probabilities, but that didn't seem to do anything.
Then I ran a 64 state PEcluster using the actual probabilities, except that all probabilities
less than 1.0e-20 were set to zero.  I got 40 non-zero states, with some interesting output
from Model.jl, but I'm wondering if the KS and chisq calculations are really correct.

Sun 06 Jun 2021 10:45:37 AM PDT
Lots of house guests have slowed progress to a crawl.  I decided to try dropping KS and use chi-square for all
tail probabilities, but this requires some non-trivial re-writing of get_fts_flowsets in particular.  So I'm trying
a re-write in julia.  I found a VERY informative piece of multi-threaded julia code (prime sieve of aristophanes) so
I've been working with julia multithreading to try to parallelize get_fts_flowsets.  The main obstacle is that
the ident lookups have to be done with a dictionary, and it's not thread-safe.  So I wrote SafeDict.jl, which
was inspired by the sieve code.  It spawns a task which reads a julia Channel and does the lookup.  The main insight
was to forget about threads and just think of asynchronously running tasks, communicating via julia Channels.
Debugging was difficult, but it seems to be working on the test code now.  One problem is that my current
version of vmware has just one thread.  I think I can start up a new guest machine with multiple threads but I
haven't done it yet.

Sun 06 Jun 2021 04:36:06 PM PDT
I spent most of the day thinking about the design of get_fts_flowsets.jl.  I think I'll try the following task-centric
approach:  There will be three types of tasks: one reader, one writer, and multiple workers.  This will only work if
I can have a large number (say 50K) worker tasks active at one time, because each task will handle one net_ident at
a time.  Each task will be associated to a unique channel.  The worker (task,channel) pairs will be in an
Array{Channel}(50000). All of the tasks will be fired up by the master thread at the beginning, and they will
immediately go into an infinite loop whose first step is to take! a message from their channel.  All the messages
will be mutable structs which will have a action field (an enum describing the payload(s)) and one or more
payload fields.  Here's my current thinking for the messages:

reader messages:
ident: "open", payload = filename, 
       direction: master -> reader
ident: "data", payload = string (one line from the file)
       direction: reader -> master
ident: "eof", no payload
       direction: reader -> master
ident: error, payload = error message/data
       direction: reader -> master
Description: Reader waits for the "open" message, opens the file and enters an infinite loop "put"-ing data
messages to the master. At eof, reader closes the file, puts the eof message, and exits

parser messages:
ident: "raw": payload = line from file
       direction: master -> parser
ident: "data": payload = NetflowData struct
worker messages:
ident: "open" payload = fts duration, min no. of records to write, writer channel
ident: "data", payload = netident, data from parser
       direction: master -> worker 
ident: "error" (overflow error)
ident: "exit", payload.closed is set to true when worker exits

Description: Each worker waits for an "exit" or "data" message.  "data": If the current record count is zero,
the histogram matrix is cleared, the net_ident is stored, and the record count is set to 1. Then the net_ident and time
are checked. If the net_ident doesn't check, an overflow message is returned.  Otherwise, if time has expired
and the record count exceeds the minimum, the chi-squares are computed and the output is "put"on the write channel.
Then the histograms, record count, and net_ident are cleared.  If time has not expired, the record count and
the histograms are incremented. 
"exit": the write/clear process is executed, the payload is set to "completed" and the worker exits. 

writer messages:
ident: "open", payload = filename
       direction: master -> writer
ident: "data", payload = data to write
       direction: worker -> writer or (master -> writer)
ident: "error", payload = error message/data
       direction: writer -> worker or master

Master process:
1. Start all the tasks.
2. put an "open" message on the reader channel
3. Enter an infinite loop, waiting on the reader channel for a data, eof, or error message
3a. data:
    1. call the parser and get the time, net_ident, and data fields
    2. look up the net_ident to get the connection no. (a unique number which increments with each new net_ident)
    3. the worker no. is the connection no. modulo the total number of workers
    4. feed the parser output to the appropriate worker
3b. error:
    1. If it's an overflow error from a worker, print and error message and punt. (There were more open connections
    than workers provided. (Maybe we can increase the set of workers here?)
    2. If it's a reaad error from the input, try again
3c. eof: send an exit message to every worker, wait for all channels to close, and then exit.

Fri Jun 11 00:51:14 2021
I installed the julia package "Actors" which looks like an excellent way to do multi-threaded programming.
I downloaded the manual.

Fri Jun 11 16:19:42 2021
I changed Reader and Writer from functions to functors.  Seems to work much better.  Both passed on test_fts.flowsets.jl
I also modified the above outline for get_fts_flowsets as follows:

reader messages:
ident: "DATA", payload = string (one line from the file)
       direction: reader -> master
ident: "EOF", no payload
       direction: reader -> master
ident: ERROR, payload = error message/data
       direction: reader -> master
Description: Reader waits for the "open" message, opens the file and enters an infinite loop "put"-ing data
messages to the master. At eof, reader closes the file, puts the eof message, and exits

parser messages:
ident: "RAW": payload = line from file
       direction: master -> parser
ident: "DATA": payload = NetflowData struct
       direcgtion: parser -> master
ident: "ERROR: payload = error message/data (bad format of line)
       direction: parser -> master
       
worker messages:
ident: "OPEN" payload = fts duration, min no. of records to write, writer channel
ident: "DATA", payload = netident, data from parser
       direction: master -> worker 
ident: "ERROR" (overflow error -- fatal)
       direction: worker -> master      
ident: "QUIT"  worker writes data and exits
       direction: master -> worker
               
Description: Each worker waits for an OPEN, DATA, or QUIT message
OPEN: the data in the payload is stored, the status is set to "active", and the record count is set to zero.
DATA: If the current record count is zero,
the histogram matrix is cleared, the net_ident is stored, and the record count is set to 1. Then the net_ident and time
are checked. If the net_ident doesn't check, an overflow message is returned.  Otherwise, if time has expired
and the record count exceeds the minimum, the chi-squares are computed and the output is "put"on the write channel.
Then the histograms, record count, and net_ident are cleared.  If time has not expired, the record count and
the histograms are incremented. 
QUIT: the write/clear process is executed, the status is set to "inactive" and the worker exits. 

writer messages:
ident: "DATA", payload = data to write
       direction: worker -> writer or (master -> writer)
ident: QUIT
       direction: master->writer (close the file, send EOF and exit)
ident: EOF
       direction: writer -> master
ident: ERROR, payload = error message/data
       direction: writer -> worker or master

Master process:
1. Start all the tasks.
2. Enter an infinite loop, waiting on the reader channel for a DATA, EOF, or ERROR message
2a. DATA:
    1. call the parser and get the time, net_ident, and data fields
    2. look up the net_ident to get the connection no. (a unique number which increments with each new net_ident)
    3. the worker no. is the connection no. modulo the total number of workers
    4. feed the parser output to the appropriate worker
2b. ERROR:
    2. try again or quit

2c. EOF: send a QUIT message to every worker and the writer, wait for all channels to close, and then exit.

Mon Jun 14 16:59:30 2021
While coding up the Worker functor, I noticed that there's a bug in the original get_fts_flowsets logic. Namely, nrecs
depends on whether it's an input parameter or an output parameter.  That could explain why all my tail probs
seem so small in the C++ version (nrecs is roughly twice the size it should be)

Mon Jun 14 18:44:17 2021
OK, I've got something coded in test_fts_flowsets to test the Worker functor.  I'll try it after lunch.

Wed Jun 16 23:40:21 2021
Well, I've tried just about everything I can think of, and the code just hangs.  Looks like a deadlock
somewhere, but I've also noticed that I'm not getting any error messages from code running on a thread.
So I'm committing what I've got now, and moving on.

Fri Jun 18 21:19:35 2021
Well, I changed the test3.jl code to be exactly the same as the docs, and it
ran, both using async (1 thread) and spawn (4 threads).  Then I modified
worker to take a worker number and a channel, and changed jobs to an array
of 4 channels.  I spawned worker with a calling sequence of i,jobs[i] and it
still worked.  Next: launch a writer and send the output from the workers
to the writer.

Mon Jun 21 21:20:54 2021
OK, I launched a writer, giving it an open stream, and it received output from 4 workers
and printed it.  Not only that, I enclosed everything in a function main() .... end to
get out of global scope and more closely approximate the real code, and miracle!  everything
still worked.  Next: modify get/test fts_flowsets to operate the same way.

Tue 22 Jun 2021 02:02:41 PM PDT
I've deleted test_fts_flowsets and moved get_fts_flowsets.jl to fts_flowsets.jlh.  Going forward, I'm going
to use .jlh for julia header files that are not intended to be executed, but only included.  fts_flowsets.jl
has been (preliminarily) coded, except for the parser.

Thu 24 Jun 2021 03:44:58 PM PDT
In the thick of debugging.  get_buckets works, and there's a fake buckets file buckets.txt.  writer launches
properly, reads a line of test output from the channel, and writes it out.  Next up is worker.  Here
I launch 2 workers and return, but neither one actually prints a debuggining message that it has been launched.
Not sure how this can happen.

----------------------------------
Sat Jul 22 13:31:53 2023
I've returned to julia programming.  Already wrote a struct AtA, but haven't written AtA.reduce yet.  AtA.add_row
seems to be running.  The result is the upper triangular Cholesky matrix.  NOTE:  I don't need to save the row rotations
because they disappear when I compute C^tC.  Actually, I rarely need to actually compute C^C, because I can compute the probability of any vector v via p = (Cv)^t(Cv).

For AtA.reduce, I need to first rotate AtA.C to upper bi-diagonal form.  This  basically consists of  column rotations,
with an occasional row rotation to correct for a sub-diagonal non-zero.  Then I need to iteratively zero out the
superdiagonal.  The resulting diagonal matrix contains the square-roots of the eigenvectors of C^tC and resulting
matrix of column rotations ("V" in the UAV notation) has the eigenvectors of C^tC as its corresponding columns.

Thu Jul 27 17:15:10 2023
OK, I've completed coding  reduce step 1 (rotate to upper bi-diagonal form) and it is working.  I've coded step 2
(iterate to diagonal form) but no checkout yet.

Sun Jul 30 12:56:57 2023
reduce is now working and it checks out!  Amazing.  I've stripped out all the debugging prints and I'm committing
it now.

Mon Aug 21 16:28:51 2023
I coded and checked out function QR(A::Matrix) and found a
couple of bugs in AtA.jl (which might still be there, BTW).  I'm
having a problem getting the mean and variance computations
base on incremental QR (see function add_row) to agree with a) Welford, and b) naive calculation.  I'm in the middle of some debugging here.  I want to apply function QR to a matrix whose first column is all ones to get the mean and mean-centered covariance matrix ala qr_reg.pdf.  No luck yet.

Mon Aug 28 13:44:50 2023
The incremental QR method didn't work for the (mean-centered) covariance matrix.  So I'm going to use Welford for
now.  Here's an outline of Cluster.jl:


mutable struct Clusters
  nclusters::Int64
  welford::Array{Welford} # one Welford struct for each cluster
  
  function reset(....)
end

function update(c::Clusters, feature_vec::Array{Float64})
  weight = [prob(welford[i],feature_vec) for i in 1:c.nclusters]
  for i in 1:c.nclusters
    update(c.welford[i],feature_vec,weight[i])
  end
end

function finalize(c::Clusters)
  cholesky = [QR(c.welford[i].covariance) for i in 1:c.nclusters]
  inv_cholesky = [inv(cholesky) for i in 1:c.nclusters]
  
  
while (feature_vec = get_feature_vec() != Nothing) #main loop




2. Using the scores from 1) as weights, feed the feature vectors to each welford
3. 

Mon Sep  4 21:33:28 2023
I coded up SVD.jl but it's not checked out yet.  I wrote a new version SVDcomp1 which I think
is more efficient. (The old version SVDcomp was hard to code, so I gave up and recoded with SVDcomp1.
The difference is the use of the "big matrix" X, which allows the row and column rotations to be automagically
saved.
NOTE:Due to quirks of Julia, A[rows,cols] .= B didn't seem to work yesterday.
Hmm, just tried it again in the REPL and it works!  I wasted hours yesterday on this.  Bah!!!!
